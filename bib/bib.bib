@unpublished{Rossi2019b,
  title = {{Walsh-Hadamard Variational Inference for Bayesian Deep Learning}},
  author  = {{Simone Rossi} and Sébastien Marmin and Maurizio Filippone},
  booktitle = 	 {{arxiv: 1905:11248}},
  abstract = {{Over-parameterized models, such as DeepNets and ConvNets, form a class of models that are routinely adopted in a wide variety of applications, and for which Bayesian inference is desirable but extremely challenging. Variational inference offers the tools to tackle this challenge in a scalable way and with some degree of flexibility on the approximation, but for over-parameterized models this is challenging due to the over-regularization property of the variational objective. Inspired by the literature on kernel methods, and in particular on structured approximations of distributions of random matrices, this paper proposes Walsh-Hadamard Variational Inference (WHVI), which uses Walsh-Hadamard-based factorization strategies to reduce the parameterization and accelerate computations, thus avoiding over-regularization issues with the variational objective. Extensive theoretical and empirical analyses demonstrate that WHVI yields considerable speedups and model reductions compared to other techniques to carry out approximate inference for over-parameterized models, and ultimately show how advances in kernel methods can be translated into advances in approximate Bayesian inference.}},
  year = 	 {2019},
}

@unpublished{ohana-2019-kernel,
    title={Kernel computations from large-scale random features obtained by Optical Processing Units},
    author={Ruben Ohana and Jonas Wacker and Jonathan Dong and Sébastien Marmin and Florent Krzakala and Maurizio Filippone and Laurent Daudet},
    year={2019},
    booktitle = {{arxiv: 1910.09880}},
    abstract = {{Approximating kernel functions with random features (RFs) has been a successful application of random projections for nonparametric estimation. However, performing random projections presents computational challenges for large-scale problems. Recently, a new optical hardware called Optical Processing Unit (OPU) has been developed for fast and energy-efficient computation of large-scale RFs in the analog domain. More specifically, the OPU performs the multiplication of input vectors by a large random matrix with complex-valued i.i.d. Gaussian entries, followed by the application of an element-wise squared absolute value operation – this last nonlinearity being intrinsic to the sensing process.  In this paper,  we show that this operation results in a dot-product kernel that has connections to the polynomial kernel, and we extend this computation to arbitrary powers of the feature map. Experiments demonstrate that the OPU kernel and its RF approximation achieve competitive performance in applications using kernel ridge regression and transfer learning for image classification. Crucially, thanks to the use of the OPU, these results are obtained with time and energy savings.}},
    keywords = {{Kernel methods, nonparametric estimation, optical computing, random features, kernel ridge regression.}}
}

@presentation{UNINE2018,
    title={A view on calibration and optimisation of computer models with Gaussian process emulators},
    author={Sébastien Marmin},
    year={2018},
    booktitle = {{Présentation à l'Institute of Statistics of University of Neuchâtel}},
    keywords = {{calibration of computer models, random features, variational inference, discrepancy}},
    coor  = {47.000435, 6.949718}
}
